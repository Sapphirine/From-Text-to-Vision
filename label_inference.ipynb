{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##label inference based on word vectors and image clustering\n",
    "##hints:\n",
    "##if you intend to use word embedding, please refer to the script text_processing_word_embedding to train\n",
    "##if you prefer to GloVe, please refer to (https://github.com/stanfordnlp/GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vectors=np.load(\"word_vector.npy\")\n",
    "dictionary=np.load(\"dictionary.npy\").item()\n",
    "reverse_dictionary=np.load(\"reverse_dictionary.npy\").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## load captions\n",
    "f=open(\"annotations/captions_on_images.txt\")\n",
    "text=f.readlines()\n",
    "num=0;\n",
    "cap={}\n",
    "for line in text:\n",
    "    if (num==0):\n",
    "        im_id=int(line)\n",
    "        cap[im_id]=\"\"\n",
    "        num=1\n",
    "        continue\n",
    "    if (line[0]=='*'):\n",
    "        num=0\n",
    "        continue\n",
    "    cap[im_id]+=line\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##load labels\n",
    "labels=[\n",
    "'airplane',\n",
    " 'apple',\n",
    " 'backpack',\n",
    " 'banana',\n",
    " 'bat',\n",
    " 'glove',\n",
    " 'bear',\n",
    " 'bed',\n",
    " 'bench',\n",
    " 'bicycle',\n",
    " 'bird',\n",
    " 'boat',\n",
    " 'book',\n",
    " 'bottle',\n",
    " 'bowl',\n",
    " 'broccoli',\n",
    " 'bus',\n",
    " 'cake',\n",
    " 'car',\n",
    " 'carrot',\n",
    " 'cat',\n",
    " 'phone',\n",
    " 'chair',\n",
    " 'clock',\n",
    " 'couch',\n",
    " 'cow',\n",
    " 'cup',\n",
    " 'table',\n",
    " 'dog',\n",
    " 'donut',\n",
    " 'elephant',\n",
    " 'hydrant',\n",
    " 'fork',\n",
    " 'frisbee',\n",
    " 'giraffe',\n",
    " 'drier',\n",
    " 'handbag',\n",
    " 'horse',\n",
    " 'hot',\n",
    " 'keyboard',\n",
    " 'kite',\n",
    " 'knife',\n",
    " 'laptop',\n",
    " 'microwave',\n",
    " 'motorcycle',\n",
    " 'mouse',\n",
    " 'orange',\n",
    " 'oven',\n",
    " 'parking',\n",
    " 'person',\n",
    " 'pizza',\n",
    " 'plant',\n",
    " 'refrigerator',\n",
    " 'remote',\n",
    " 'sandwich',\n",
    " 'scissors',\n",
    " 'sheep',\n",
    " 'sink',\n",
    " 'skateboard',\n",
    " 'skis',\n",
    " 'snowboard',\n",
    " 'spoon',\n",
    " 'ball',\n",
    " 'stop',\n",
    " 'suitcase',\n",
    " 'surfboard',\n",
    " 'teddy',\n",
    " 'tennis',\n",
    " 'tie',\n",
    " 'toaster',\n",
    " 'toilet',\n",
    " 'toothbrush',\n",
    " 'traffic',\n",
    " 'train',\n",
    " 'truck',\n",
    " 'tv',\n",
    " 'umbrella',\n",
    " 'vase',\n",
    " 'wine',\n",
    " 'zebra']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##load stopwords\n",
    "f=open(\"annotations/stopwords.txt\")\n",
    "text=f.readlines()\n",
    "stw=set()\n",
    "for line in text:\n",
    "    stw.add(line[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##labels id in dictionary\n",
    "l_id=[]\n",
    "for x in labels:\n",
    "    l_id.append(dictionary[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##compute the label distribution on text for each original image\n",
    "l_dis={}##label distribution on captions\n",
    "for idx in cap.keys():\n",
    "    lis=cap[idx].split()\n",
    "    vote=[0]*len(labels)\n",
    "    for word in lis:\n",
    "        ##clean words and only consider those which are not stop words\n",
    "        w=\"\".join(l for l in word if l not in {',','.','!','?','\\\"','(',')','\\''})\n",
    "        if w in stw:\n",
    "            continue\n",
    "        try:\n",
    "            w_id=dictionary[w]\n",
    "        except:\n",
    "            continue\n",
    "        vec=vectors[w_id]\n",
    "        ##distribution for each word\n",
    "        v1=[0]*len(labels)\n",
    "        for i in range(0,len(labels)):\n",
    "            cate=c_id[i]\n",
    "            vec2=vectors[cate]\n",
    "            sim_vec=vec*vec2\n",
    "            cos_sim=sum(sim_vec)\n",
    "            v1[i]=cos_sim\n",
    "        nsum=sum(v1)\n",
    "        for i in range(0,len(labels)):\n",
    "            vote[i]+=v1[i]/nsum\n",
    "    nsum=sum(vote)\n",
    "    l_dis[idx]=np.array(vote)/nsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##compute the label distribution on clusters\n",
    "num=0\n",
    "c_dis={}##label distribution of clusters \n",
    "n_clu=100##number of clusters\n",
    "c_dic={}\n",
    "for i in range(0,n_clu):\n",
    "    c_dis[i]=[0]*len(labels)\n",
    "inp=open(\"clusters.txt\")\n",
    "for line in inp.readlines():\n",
    "    my=line.split()\n",
    "    c_dic[my[0]]=int(my[1])\n",
    "inp.close()\n",
    "for img in c_dic:\n",
    "    sup=img.find(\"_\")\n",
    "    idx=int(img[:sup])\n",
    "    cluster=c_dic[img]\n",
    "    for i in range(0,len(labels)):\n",
    "        c_dis[cluster][i]+=l_dis[idx][i]\n",
    "for i in range(0,n_clu):\n",
    "    c_dis[i]=np.array(c_dis[i])\n",
    "    nsum=sum(c_dis[i])\n",
    "    if nsum>0:\n",
    "        c_dis[i]=c_dis[i]/nsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##scoring\n",
    "alpha=0.5\n",
    "final_score={}\n",
    "for img in c_dic:\n",
    "    sup=img.find(\"_\")\n",
    "    idx=int(img[:sup])\n",
    "    cluster=c_dic[img]\n",
    "    final_score[img]=alpha*c_dis[cluster]+(1-alpha)*l_dis[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##label with highest final score for one image\n",
    "final_score[img].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
