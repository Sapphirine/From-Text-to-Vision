{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##intergrating the text set\n",
    "##here is a small text sample\n",
    "##if you want to use good embedding, please try to download wikipedia\n",
    "f=open(\"sis/annotation.txt\")\n",
    "text=f.readlines()\n",
    "words=[]\n",
    "dic=set()\n",
    "for line in text:\n",
    "    word=line.split()\n",
    "    for w in word:\n",
    "        if w.isalnum():\n",
    "            dic.add(w)\n",
    "            words.append(w)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f=open(\"caption/annotation.txt\")\n",
    "text=f.readlines()\n",
    "for line in text:\n",
    "    word=line.split()\n",
    "    for w in word:\n",
    "        if w.isalnum():\n",
    "            dic.add(w)\n",
    "            words.append(w)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f=open(\"annotations/captions.txt\")\n",
    "text=f.readlines()\n",
    "for line in text:\n",
    "    word=line.split()\n",
    "    for w in word:\n",
    "        w=\"\".join(l for l in w if l not in {',','.','!','?','\\\"','(',')','\\''})\n",
    "        if len(w)>0:\n",
    "            dic.add(w)\n",
    "            words.append(w)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size=len(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37119"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##dataset construction\n",
    "import collections\n",
    "import math\n",
    "def build_dataset(words, vocabulary_size):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 1], ('a', 275859), ('the', 210048), ('to', 87244), ('and', 80056)]\n",
      "Sample data [50, 3363, 235, 7, 212, 9, 24, 3, 61, 4028] ['our', 'landmark', 'tree', 'in', 'town', 'was', 'about', 'to', 'be', 'destroyed']\n"
     ]
    }
   ],
   "source": [
    "## Build the dictionary and replace rare words with UNK token.\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words, vocabulary_size)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "data_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##generate batch for training\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "        # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Build and train a skip-gram model.\n",
    "\n",
    "batch_size = 16\n",
    "embedding_size = 100  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/cpu:0'):\n",
    "    # Look up embeddings for inputs.\n",
    "        embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "    # time we evaluate the loss.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                        biases=nce_biases,\n",
    "                         labels=train_labels,\n",
    "                         inputs=embed,\n",
    "                         num_sampled=num_sampled,\n",
    "                         num_classes=vocabulary_size))\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "        normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(\n",
    "        valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  251.863830566\n",
      "Nearest to that: sigils, caster, faring, chun, fprest, basically, trustees, saphire,\n",
      "Nearest to some: 747, jonah, adrift, greets, era, biplane, joystick, mighty,\n",
      "Nearest to see: facepaint, atlas, menus, woozy, rw, richest, baseline, permission,\n",
      "Nearest to the: gong, solutions, badmitton, hungout, brodcosting, storybooks, ufc, coursework,\n",
      "Nearest to while: bedridden, papered, supercell, installments, lump, searches, pleaded, repeating,\n",
      "Nearest to water: reviews, call, miserables, 6am, gaurds, goup, siding, filth,\n",
      "Nearest to everyone: unamerican, keytar, compose, incorrect, bridges, stick, frazzled, deliveries,\n",
      "Nearest to talking: dwellers, invoices, squaring, refinished, malt, wilkes, vesel, pirate,\n",
      "Nearest to food: uploaded, lavene, movable, doctors, deeply, sons, replicas, geodesic,\n",
      "Nearest to had: realize, latches, casualness, borrowing, sadness, fossmeetnitc, rockers, recreations,\n",
      "Nearest to went: 5505, difernt, regaining, loews, floaties, ktop, mumbling, resides,\n",
      "Nearest to little: fluently, irish, critics, contents, grant, above-ground, hey, vandelay,\n",
      "Nearest to into: atlk, instrument, 55, sacrifices, warthogs, abbeys, michrophones, tiered,\n",
      "Nearest to at: modernity, jefferson, hurdel, swabbing, 44, soldering, console, fritters,\n",
      "Nearest to as: sucessful, overhead, prettily, piglet, tava, signage, lend, dungeon,\n",
      "Nearest to then: klingon, elite, kilts, wive, mature, froze, grooved, homels,\n",
      "Average loss at step  2000 :  100.604518335\n",
      "Average loss at step  4000 :  43.1889289285\n",
      "Average loss at step  6000 :  28.3484124324\n",
      "Average loss at step  8000 :  20.2430734969\n",
      "Average loss at step  10000 :  16.5660517848\n",
      "Nearest to that: records, gaze, rollerbladed, furiously, slight, it, dead, heck,\n",
      "Nearest to some: cued, progressed, explain, village, awake, airport, embers, kite,\n",
      "Nearest to see: senior, gap, offbeat, denial, technical, veterans, sean, listeners,\n",
      "Nearest to the: card, riddles, vying, sailed, german, nose, superheroes, confessed,\n",
      "Nearest to while: therefore, bears, arr, shower, yolk, childhood, underway, provided,\n",
      "Nearest to water: pre, athletes, middle, waste, angry, safely, saucer, representative,\n",
      "Nearest to everyone: graduate, baptism, nerdo, gone, marry, chair, woop, affections,\n",
      "Nearest to talking: ultimate, visitors, tint, 1300, muddy, symerton, copy, emmy,\n",
      "Nearest to food: roads, pet, frosting, farming, sings, fearful, ordering, robe,\n",
      "Nearest to had: have, chill, gap, landing, lie, advancements, buddha, darken,\n",
      "Nearest to went: scrape, go, convertibles, lay, picnics, riddles, racer, acres,\n",
      "Nearest to little: pal, 1500, capped, confident, monster, covering, lost, sing,\n",
      "Nearest to into: accounting, drums, rice, watermelon, puddles, amp, myself, buys,\n",
      "Nearest to at: pelicans, trailing, parked, walls, inebriated, interested, sweetheart, formation,\n",
      "Nearest to as: informative, exception, joining, glowed, african, coins, slipped, receives,\n",
      "Nearest to then: cakes, candle, tinsel, afterward, fallen, fertilize, apple, highway,\n",
      "Average loss at step  12000 :  14.8142284598\n",
      "Average loss at step  14000 :  12.2962699329\n",
      "Average loss at step  16000 :  10.5156471477\n",
      "Average loss at step  18000 :  9.85557076848\n",
      "Average loss at step  20000 :  10.051614115\n",
      "Nearest to that: hook, francisco, certainty, stan, relay, captured, lesbian, galaxies,\n",
      "Nearest to some: trolly, approval, many, jones, overnight, coochie, lover, sops,\n",
      "Nearest to see: do, braces, get, beverages, jenga, mingle, veterans, listeners,\n",
      "Nearest to the: washed, loading, nose, rebounds, superheroes, pestle, teddy, follwed,\n",
      "Nearest to while: yelled, cheerleading, kayak, volley, books, rhino, kart, purse,\n",
      "Nearest to water: congratulations, legal, trashed, accounting, doughnut, stan, racers, disburse,\n",
      "Nearest to everyone: farming, rascals, robe, launch, meadow, hockey, gone, nightfall,\n",
      "Nearest to talking: unnaturally, ultimate, 1300, tint, insect, bagel, consisted, visitors,\n",
      "Nearest to food: roads, resturant, coliseum, jonna, aspects, racers, scratch, saucer,\n",
      "Nearest to had: awakes, matched, got, continue, spa, saw, success, hospital,\n",
      "Nearest to went: got, decided, galore, darken, racer, blackboard, adorn, platter,\n",
      "Nearest to little: signpost, series, confident, supporters, io, castles, dining, nasty,\n",
      "Nearest to into: to, actor, proposing, trampoline, bronco, ages, down, patrons,\n",
      "Nearest to at: spaceship, unfamilar, pooping, pilots, jones, fenway, diversion, dying,\n",
      "Nearest to as: leads, instructor, marked, bucking, oink, exception, stronger, momentous,\n",
      "Nearest to then: deadly, inebriated, momentous, misty, interactive, th, highway, capped,\n",
      "Average loss at step  22000 :  8.1862353301\n",
      "Average loss at step  24000 :  8.3712714287\n",
      "Average loss at step  26000 :  8.77310368967\n",
      "Average loss at step  28000 :  9.39735075152\n",
      "Average loss at step  30000 :  7.97227509928\n",
      "Nearest to that: overcast, basic, swan, skyskrapers, rad, fortune, eateries, cheap,\n",
      "Nearest to some: divers, hundreds, awesomely, sweater, oink, belt, feeds, showcasing,\n",
      "Nearest to see: sucks, hundred, cosplay, offbeat, aim, grid, camel, billboards,\n",
      "Nearest to the: diversity, altogether, fred, volunteer, books, buttermilk, exchanging, journeys,\n",
      "Nearest to while: tailoring, pleasing, creation, handled, receives, revelry, plague, aladdin,\n",
      "Nearest to water: anemones, document, soliders, oooh, newborn, firecrackers, recruits, captivating,\n",
      "Nearest to everyone: caps, electronics, nurses, installed, whew, records, fenway, otherwise,\n",
      "Nearest to talking: 1300, gossip, unnaturally, consisted, 1896, tint, throwers, ultimate,\n",
      "Nearest to food: view, time, document, aspects, resturant, coliseum, coaxed, arts,\n",
      "Nearest to had: necklaces, chill, raspberries, juggled, greenhouse, suffered, jurassic, vortex,\n",
      "Nearest to went: got, go, decided, darken, came, curve, accomplishment, breaking,\n",
      "Nearest to little: foolish, io, loc, repaint, batman, highland, 3rd, greenhouse,\n",
      "Nearest to into: cousins, shallows, laws, woody, karaoke, loopy, visitors, cheeseburgers,\n",
      "Nearest to at: on, hospitals, spoon, seekers, preceded, gardeners, shopkeepers, angkor,\n",
      "Nearest to as: commercial, newest, sheepish, exerciese, craggy, gently, tasks, stronger,\n",
      "Nearest to then: orientation, fertilize, interactive, pee, munch, kayak, shrubbery, bouncing,\n",
      "Average loss at step  32000 :  8.39537968886\n",
      "Average loss at step  34000 :  7.53762899685\n",
      "Average loss at step  36000 :  7.31060318917\n",
      "Average loss at step  38000 :  7.24831206656\n",
      "Average loss at step  40000 :  7.20768196332\n",
      "Nearest to that: when, iced, exchange, soooo, exemplified, pinata, finishing, trolley,\n",
      "Nearest to some: many, toll, circled, booed, woooooahhhh, endeavor, chiseled, conquered,\n",
      "Nearest to see: offbeat, jewel, stronger, grid, foreigners, dine, leaping, mild,\n",
      "Nearest to the: superheroes, instruct, lengths, skateboard, streaked, whore, fir, caravel,\n",
      "Nearest to while: fred, guinnes, beable, tournaments, unannounced, haunted, worries, collide,\n",
      "Nearest to water: apartment, nailed, horsemen, carosel, exhilerating, exterior, guitars, bumber,\n",
      "Nearest to everyone: elements, conquered, sempme, mamba, powerless, plethora, undrinkable, islands,\n",
      "Nearest to talking: gossip, unnaturally, consisted, throwers, 1896, quenches, tint, attracts,\n",
      "Nearest to food: sweetest, specular, scooter, recharging, epic, scen, photographs, pet,\n",
      "Nearest to had: have, rendition, cable, slept, wonders, connecting, raffle, videos,\n",
      "Nearest to went: go, amazes, plunge, shred, sunbath, buttermilk, wrap, accomplishment,\n",
      "Nearest to little: earned, gently, geodesic, aid, series, kangaroo, repaint, pleasantly,\n",
      "Nearest to into: rulings, rice, ensemble, mannered, sweat, uses, muffler, cascaded,\n",
      "Nearest to at: worries, answer, adopting, inebriated, snacking, specimens, volume, searches,\n",
      "Nearest to as: marrying, plunge, gently, sempme, rule, fixer, tasks, rhino,\n",
      "Nearest to then: when, purse, katra, interactive, prime, tournaments, books, encircling,\n",
      "Average loss at step  42000 :  7.03403381824\n",
      "Average loss at step  44000 :  6.54965723801\n",
      "Average loss at step  46000 :  6.75427824283\n",
      "Average loss at step  48000 :  7.57142566204\n",
      "Average loss at step  50000 :  7.00167492425\n",
      "Nearest to that: ramcharger, pee, exchange, beck, aligning, terminal, tying, nugget,\n",
      "Nearest to some: layers, soundboard, mosquitos, awake, oktober, graffitied, horsemen, branches,\n",
      "Nearest to see: find, take, watch, marry, zero, enjoy, gethers, assistant,\n",
      "Nearest to the: dissent, satisfy, stored, specialty, clapping, ignored, favored, extend,\n",
      "Nearest to while: causing, grooves, comeback, roundtable, wiring, pinata, collide, click,\n",
      "Nearest to water: beach, holidays, gameplan, robots, doritos, jaunt, trestle, exhilerating,\n",
      "Nearest to everyone: electronics, trolley, mamba, greeting, delays, extensive, jogged, undrinkable,\n",
      "Nearest to talking: throwers, gossip, quenches, coordinated, wrestler, unnaturally, 1896, cutie,\n",
      "Nearest to food: charms, hiker, scooter, stalkers, autumn, robots, nearing, prosperous,\n",
      "Nearest to had: overexposed, have, scouting, rendition, strategies, breaking, deadly, bearable,\n",
      "Nearest to went: walked, scrape, decided, go, consulted, refused, receding, grows,\n",
      "Nearest to little: championship, geodesic, aid, series, connecting, kangaroo, relented, titanic,\n",
      "Nearest to into: mannered, coves, cascaded, muffler, bread, fulfilled, marajuana, reflective,\n",
      "Nearest to at: by, on, expedition, over, slope, mamba, jeeps, momentous,\n",
      "Nearest to as: when, cartridges, hotels, scuttled, firecrackers, scanned, achieve, wrinkles,\n",
      "Nearest to then: hut, provides, interactive, pastures, pinata, beep, parasailing, mischief,\n",
      "Average loss at step  52000 :  7.51807811558\n",
      "Average loss at step  54000 :  7.00596804929\n",
      "Average loss at step  56000 :  6.52029191315\n",
      "Average loss at step  58000 :  6.82560416973\n",
      "Average loss at step  60000 :  6.57161583078\n",
      "Nearest to that: bassinet, raffle, johnsfirst, ramcharger, momentous, 1am, aligning, diamond,\n",
      "Nearest to some: any, errant, fools, satisfy, greats, mosquitos, fascinate, imperative,\n",
      "Nearest to see: find, watch, offbeat, eat, torah, loose, dusting, misfit,\n",
      "Nearest to the: bynum, subside, sprained, excellence, ws, loading, pinchy, acapella,\n",
      "Nearest to while: nos, apply, trailed, cupful, after, billion, stronger, selfish,\n",
      "Nearest to water: beach, robots, gameplan, files, stupid, streaming, doritos, amplified,\n",
      "Nearest to everyone: nurses, golfland, fenway, acapella, michigand, holes, awed, healed,\n",
      "Nearest to talking: quenches, wrestler, cutie, gossip, coordinated, bout, throwers, rehashed,\n",
      "Nearest to food: buttons, segment, suitcase, beach, roadblocks, valedictorian, pastime, unlock,\n",
      "Nearest to had: have, captured, constantine, highly, teh, goose, corndogs, unannounced,\n",
      "Nearest to went: go, decided, walked, came, attract, traversing, via, pretend,\n",
      "Nearest to little: series, camper, toddler, downing, stricken, darkens, repaired, championship,\n",
      "Nearest to into: basin, dryer, caring, mannered, unhappy, precede, rulings, punk,\n",
      "Nearest to at: occupies, vineyards, circled, depps, coins, hospitals, fixture, intricate,\n",
      "Nearest to as: beable, bucking, foolish, then, pinata, heckled, smashed, aladdin,\n",
      "Nearest to then: afterwards, when, tempts, acquaintance, acapella, dove, pinata, galaxies,\n",
      "Average loss at step  62000 :  6.53734615123\n",
      "Average loss at step  64000 :  7.7813722235\n",
      "Average loss at step  66000 :  7.92992839301\n",
      "Average loss at step  68000 :  7.19390420449\n",
      "Average loss at step  70000 :  6.83758437896\n",
      "Nearest to that: interacted, permission, orchestra, raw, cougas, spectacualr, 1am, lectured,\n",
      "Nearest to some: loading, crowdsurfing, coordinate, jones, strings, revelers, lots, majestically,\n",
      "Nearest to see: enjoy, eat, tripped, www, watch, hieroglyphs, laidback, torah,\n",
      "Nearest to the: cannonballs, gulley, hazing, slamming, favourite, pickles, chicks, sponsoring,\n",
      "Nearest to while: sue, secrets, trailed, fertilize, struggle, deals, after, comeback,\n",
      "Nearest to water: beach, robots, strutted, flattering, pews, splashes, amplified, beliefs,\n",
      "Nearest to everyone: impress, dame, browned, showered, simpsons, doctor, grrrrrr, obstacles,\n",
      "Nearest to talking: quenches, gossip, sax, giraffes, wards, coordinated, tint, write,\n",
      "Nearest to food: drinks, noisy, rice, overrun, jaunt, purposes, herd, edwards,\n",
      "Nearest to had: have, shrines, bathed, caller, cutouts, bouncing, tornado, pinwheels,\n",
      "Nearest to went: decided, toasted, accomplishment, installation, got, scampered, come, put,\n",
      "Nearest to little: downside, darkens, institute, brownstones, cyclers, foud, camper, greenhouse,\n",
      "Nearest to into: seekers, caring, preschool, wills, exterior, shoulders, spoon, unhappy,\n",
      "Nearest to at: universal, football, avoided, near, ii, offense, ghostly, conducting,\n",
      "Nearest to as: after, narnia, perspectives, when, so, jogger, humble, aladdin,\n",
      "Nearest to then: schoolyard, funhouse, kart, executive, antler, eaters, exotics, unannounced,\n",
      "Average loss at step  72000 :  6.74443244839\n",
      "Average loss at step  74000 :  6.81521123332\n",
      "Average loss at step  76000 :  6.3926738323\n",
      "Average loss at step  78000 :  6.71682792234\n",
      "Average loss at step  80000 :  6.81880511332\n",
      "Nearest to that: streetswere, lovebirds, intensify, caricature, assess, dynasty, bassinet, erosion,\n",
      "Nearest to some: notra, barrels, jones, significant, ropes, stumbles, convos, woooooahhhh,\n",
      "Nearest to see: watch, find, get, make, eat, enjoy, mild, offbeat,\n",
      "Nearest to the: cannonballs, snuck, gulley, sailors, volunteer, spotlights, petty, cannons,\n",
      "Nearest to while: script, scrimmage, mountie, ourtoors, representatives, bb, equal, swapped,\n",
      "Nearest to water: parade, formalities, beliefs, buggy, queue, crowd, pastime, layout,\n",
      "Nearest to everyone: impress, plethora, browned, holes, showered, installed, dame, declared,\n",
      "Nearest to talking: quenches, gossip, giraffes, sax, refinished, 1896, confused, throwers,\n",
      "Nearest to food: jaunt, sweethearts, noisy, aid, orchestra, enemies, operate, correct,\n",
      "Nearest to had: have, rendition, actress, challenging, nos, shouted, ferociously, squeee,\n",
      "Nearest to went: came, decided, got, headed, walked, stopped, gathered, trixie,\n",
      "Nearest to little: medal, derelict, failed, reviewing, camper, institute, precious, min,\n",
      "Nearest to into: shoulders, spoon, trampoline, inclusive, hearken, mannered, sun, calendars,\n",
      "Nearest to at: spotter, ghostly, wash, institute, combining, momentous, offense, touristy,\n",
      "Nearest to as: boe, jaw, ghostly, beable, frivolous, 3rd, dissatisfied, pup,\n",
      "Nearest to then: monks, reappear, fading, morally, sailed, galloping, erosion, mitch,\n",
      "Average loss at step  82000 :  6.35377463996\n",
      "Average loss at step  84000 :  6.45499448919\n",
      "Average loss at step  86000 :  6.3311314832\n",
      "Average loss at step  88000 :  6.36561081719\n",
      "Average loss at step  90000 :  6.56652001476\n",
      "Nearest to that: rainstorm, 1am, outs, champs, residences, gaze, arching, zebra,\n",
      "Nearest to some: woooooahhhh, painters, plying, bars, luckiest, rehearses, celebratory, buggies,\n",
      "Nearest to see: celebrate, offbeat, relationships, yolk, edown, bridesmade, varun, leaps,\n",
      "Nearest to the: majestically, tugged, feild, scored, volunteer, intensify, drool, superheroes,\n",
      "Nearest to while: scaled, scrimmage, moustach, causing, garnish, repaint, chords, sue,\n",
      "Nearest to water: shore, pier, beach, boobs, muscle, blasting, glacier, coldest,\n",
      "Nearest to everyone: flocked, rainboots, gentlemen, chicks, automobile, productive, scrapers, 26,\n",
      "Nearest to talking: quenches, 911, giraffes, confused, tint, unlock, funnels, coliseum,\n",
      "Nearest to food: jaunt, confederates, orchestra, hookah, vets, pulpit, noticing, sign,\n",
      "Nearest to had: freshmen, enters, carinal, reappear, brews, yer, marked, bathed,\n",
      "Nearest to went: go, decided, trixie, headed, shivered, amazes, glued, acknowledged,\n",
      "Nearest to little: institute, darkens, wouldnt, young, wm, outsider, camper, buidlings,\n",
      "Nearest to into: reflective, arson, spoon, tasks, across, varun, wash, caring,\n",
      "Nearest to at: jester, after, escapades, regarding, spoon, mice, scholars, kat,\n",
      "Nearest to as: selfish, narnia, hotels, pretzel, repelled, ninja, smashed, scrape,\n",
      "Nearest to then: shoulda, today, mechanism, paperback, exemplified, sustained, wading, glitz,\n",
      "Average loss at step  92000 :  6.72954599309\n",
      "Average loss at step  94000 :  6.36987092626\n",
      "Average loss at step  96000 :  6.34265043557\n",
      "Average loss at step  98000 :  6.35524593937\n",
      "Average loss at step  100000 :  6.17492864799\n",
      "Nearest to that: helens, frazzled, erosion, sails, captivating, bachelor, clocks, mixing,\n",
      "Nearest to some: revelers, cityscapes, daunting, headstone, snapshots, ashy, cheeseburger, graffitied,\n",
      "Nearest to see: find, hydracodolene, offbeat, candlesticks, junkyard, crates, neglected, sinring,\n",
      "Nearest to the: bum, comging, superheroes, trixie, decorator, aloft, undaunted, blobby,\n",
      "Nearest to while: jed, scrimmage, exceptional, coax, scaled, repaint, deocrating, receives,\n",
      "Nearest to water: city, analyzed, boobs, architecture, beach, catchy, outdone, festival,\n",
      "Nearest to everyone: adequate, steaks, 14th, bigsmile, quieted, tibetan, burkas, yoshiko,\n",
      "Nearest to talking: quenches, unlock, write, confused, hydration, wards, funnels, coliseum,\n",
      "Nearest to food: scratches, pulpit, jaunt, confederates, risk, han, yoghurt, stronger,\n",
      "Nearest to had: have, has, chills, enters, popeye, enchiladas, grooming, broom,\n",
      "Nearest to went: go, decided, tried, grain, got, trixie, jenga, threaded,\n",
      "Nearest to little: perceptive, large, upstream, wm, shallow, overhang, precious, real,\n",
      "Nearest to into: through, spoon, down, reflective, garnished, fullest, preschool, cherries,\n",
      "Nearest to at: lei, elegance, nearing, marti, savant, recalled, bicycler, la,\n",
      "Nearest to as: repelled, bayou, genealogy, jogger, longs, gazed, smashed, signals,\n",
      "Nearest to then: loading, monks, lottery, stationed, pee, craggy, electronics, blizzard,\n",
      "Average loss at step  102000 :  6.06356630409\n",
      "Average loss at step  104000 :  6.39089123595\n",
      "Average loss at step  106000 :  6.58052287412\n",
      "Average loss at step  108000 :  6.05445554733\n",
      "Average loss at step  110000 :  6.27489323735\n",
      "Nearest to that: momentous, pitchfork, ramcharger, episode, interlude, mantel, cuts, lingered,\n",
      "Nearest to some: leek, caravan, graffitied, depressing, snapshots, outhouse, jones, woooooahhhh,\n",
      "Nearest to see: visit, get, keep, crates, watch, forgot, mild, take,\n",
      "Nearest to the: soundboard, abit, baduah, meditated, funnier, bro, permission, cavities,\n",
      "Nearest to while: unhappy, ventilate, foolish, departure, admires, soooo, fearlessly, curiouser,\n",
      "Nearest to water: beach, boobs, rice, crowd, analyzed, architecture, catchy, slamming,\n",
      "Nearest to everyone: quenches, sabina, finisher, thou, impress, perfomed, greg, walrus,\n",
      "Nearest to talking: quenches, write, voices, hydration, tint, puck, 911, songwriters,\n",
      "Nearest to food: deliciousness, buttons, hipsterish, unused, tummy, billion, meusem, inclusive,\n",
      "Nearest to had: took, rendition, carnevil, tar, have, receives, saw, pretend,\n",
      "Nearest to went: decided, came, go, picnics, unwind, trixie, companion, spacemen,\n",
      "Nearest to little: young, choosing, erected, mallard, finance, honest, achieving, shallow,\n",
      "Nearest to into: reflective, rive, through, pureed, preschool, fullest, tube, on,\n",
      "Nearest to at: gravesite, sainted, bycyclists, mamba, snipper, barbecuing, corporate, capitalism,\n",
      "Nearest to as: peeling, buttercup, trans, fascinate, attendant, nigh, deocrating, repelled,\n",
      "Nearest to then: surpassed, monks, swerve, electronics, cronies, snowboards, pee, carnevil,\n",
      "Average loss at step  112000 :  5.95001107705\n",
      "Average loss at step  114000 :  5.96883592808\n",
      "Average loss at step  116000 :  5.72571045625\n",
      "Average loss at step  118000 :  5.69720072782\n",
      "Average loss at step  120000 :  5.89948555982\n",
      "Nearest to that: weeds, tt, sails, blazing, golfland, yadier, julie, crushing,\n",
      "Nearest to some: caravan, loading, graffitied, snapshots, oasis, lots, prisons, gowns,\n",
      "Nearest to see: eat, visit, watch, relationships, gethers, get, find, grid,\n",
      "Nearest to the: barton, jenson, justin, bess, oktoberfest, ignored, rip, czech,\n",
      "Nearest to while: scrimmage, tj, surpassed, battled, cupful, tempo, geishas, kerioke,\n",
      "Nearest to water: catchy, possibility, beach, slamming, goods, crags, boobs, unnaturally,\n",
      "Nearest to everyone: someone, latte, plummeted, bmw, flippies, partway, delays, temperatures,\n",
      "Nearest to talking: quenches, tint, cutie, puck, confused, write, coordinated, 911,\n",
      "Nearest to food: hipsterish, sops, rest, charrades, snowflakes, goto, spits, collect,\n",
      "Nearest to had: has, have, found, actress, tearful, rendition, operate, popeye,\n",
      "Nearest to went: decided, got, wanted, walked, unimpressed, grain, jenga, exotics,\n",
      "Nearest to little: crock, bit, diverted, cascades, warrior, classified, creeks, slower,\n",
      "Nearest to into: reflective, down, outer, crowed, rims, debri, roundtable, spoon,\n",
      "Nearest to at: thingy, adopting, speedo, tanking, ghostly, nearing, bycyclists, ii,\n",
      "Nearest to as: too, vaccinated, repelled, trend, purgatory, boobs, inspecting, gameplan,\n",
      "Nearest to then: cronies, carnevil, shoulda, surpassed, swift, adventerous, lakeshore, jurassic,\n",
      "Average loss at step  122000 :  6.01878319907\n",
      "Average loss at step  124000 :  5.75595442426\n",
      "Average loss at step  126000 :  5.59782887411\n",
      "Average loss at step  128000 :  6.10354613304\n",
      "Average loss at step  130000 :  6.26321442389\n",
      "Nearest to that: pens, condom, varying, tingling, extraordinary, turbo, livens, soooo,\n",
      "Nearest to some: nectar, lots, painters, malfunctioned, woooooahhhh, gender, plying, protrusions,\n",
      "Nearest to see: eat, start, take, visit, find, do, offbeat, patriotism,\n",
      "Nearest to the: cayman, superheroes, loomed, disconcerting, cannonballs, dint, flowed, blade,\n",
      "Nearest to while: ranger, legends, cheerleading, wp, dissipating, pedraza, furrys, tackiest,\n",
      "Nearest to water: hunk, editions, sacrament, momentous, crosswords, queue, growths, anticipate,\n",
      "Nearest to everyone: everybody, darn, endeavors, spaceship, grandma, seattle, frodo, nurses,\n",
      "Nearest to talking: quenches, dancing, hydration, 911, triangle, attends, walking, tint,\n",
      "Nearest to food: sisterhood, reuse, snowflakes, coppers, apiece, spoil, plains, satisfy,\n",
      "Nearest to had: have, saw, fifties, connecting, orphan, visited, found, patriotism,\n",
      "Nearest to went: walked, decided, headed, sat, go, came, grain, itchy,\n",
      "Nearest to little: camper, cry, wm, darkens, leter, wouldnt, ralphy, infused,\n",
      "Nearest to into: reflective, roommates, opinion, beth, blisteringly, crotch, handstand, rive,\n",
      "Nearest to at: 14th, institute, ghostly, grrrrrr, haight, montage, morally, smooknag,\n",
      "Nearest to as: when, trend, deocrating, nigh, fixer, purgatory, weve, topical,\n",
      "Nearest to then: before, departing, when, monks, lox, sandbag, conservatory, julie,\n",
      "Average loss at step  132000 :  5.99469527435\n",
      "Average loss at step  134000 :  5.62666188967\n",
      "Average loss at step  136000 :  5.70556513214\n",
      "Average loss at step  138000 :  6.33937360394\n",
      "Average loss at step  140000 :  5.82982861161\n",
      "Nearest to that: everybody, proximity, agh, flooding, handprints, yadier, buidling, aligning,\n",
      "Nearest to some: lots, cords, coordinate, painters, odds, coated, owls, otuside,\n",
      "Nearest to see: eat, offbeat, try, find, do, grid, marry, unwrap,\n",
      "Nearest to the: its, repel, relics, entirety, ninjas, apache, instruct, goddess,\n",
      "Nearest to while: hated, lying, fertilize, havta, wp, deocrating, receives, susan,\n",
      "Nearest to water: trashed, fingerprinted, mare, firecrackers, emergencies, carousals, tequila, orientation,\n",
      "Nearest to everyone: everybody, strictly, libre, optimist, legitimately, supposedly, stevens, parsonage,\n",
      "Nearest to talking: talk, quenches, tint, blade, rigors, bewildered, confused, dancing,\n",
      "Nearest to food: sisterhood, apiece, snowflakes, spits, eggs, evidence, breezed, linens,\n",
      "Nearest to had: have, chills, softness, tullips, soto, peculiar, porched, revelry,\n",
      "Nearest to went: decided, go, came, tried, walked, stopped, headed, drove,\n",
      "Nearest to little: institute, awaits, camper, sweetest, partake, timber, jerked, sizzling,\n",
      "Nearest to into: worht, inebriated, fromt, through, beth, handler, comforts, ray,\n",
      "Nearest to at: joining, woodwork, coins, certainty, slickers, diversion, funnier, montage,\n",
      "Nearest to as: trend, cabernet, futher, trepidation, admires, walmart, consisting, dd,\n",
      "Nearest to then: when, hazel, phrase, wildflower, trippy, moustach, clothed, exceptional,\n",
      "Average loss at step  142000 :  5.61642972445\n",
      "Average loss at step  144000 :  5.62818252063\n",
      "Average loss at step  146000 :  5.04951332581\n",
      "Average loss at step  148000 :  5.37784520781\n",
      "Average loss at step  150000 :  5.80499654675\n",
      "Nearest to that: exasperated, cougas, climber, centerpieces, nauseated, retainer, 1950s, shortage,\n",
      "Nearest to some: trolly, several, parkour, propane, sails, kickball, cabs, otuside,\n",
      "Nearest to see: remember, eat, say, do, marry, commemorate, screams, gethers,\n",
      "Nearest to the: intensify, dismounted, crevice, journeyed, instruct, fenway, hunters, 3000,\n",
      "Nearest to while: nos, dreamt, hoodie, dove, marketing, fertilize, unwieldy, goddess,\n",
      "Nearest to water: firecrackers, wrestlers, passersby, trailing, crags, wrinkle, files, sleepyhead,\n",
      "Nearest to everyone: figuring, powerless, saucy, drags, stowed, zebra, counting, geologists,\n",
      "Nearest to talking: quenches, blade, tint, triangle, songwriters, collectibles, talk, rigors,\n",
      "Nearest to food: snowflakes, spits, apiece, eulogy, walrus, bunts, barrage, basterds,\n",
      "Nearest to had: have, pennies, songwriters, chill, aladdin, thme, ovation, alchemist,\n",
      "Nearest to went: jubilant, decided, dispersed, hemisphere, via, headed, go, skip,\n",
      "Nearest to little: infused, camper, astonished, affected, vane, baby, ap, downside,\n",
      "Nearest to into: through, towards, dunk, reflective, cheeseburgers, concise, across, bridal,\n",
      "Nearest to at: 21, spotter, ivory, distillery, parage, quieted, lilley, scantily,\n",
      "Nearest to as: repelled, overload, racetrack, switchbacks, pokemon, tasks, smashed, hoot,\n",
      "Nearest to then: fertilize, knighted, barnaby, afterwards, attracts, doorstep, deocrating, reviewing,\n",
      "Average loss at step  152000 :  5.88638739669\n",
      "Average loss at step  154000 :  5.80648223352\n",
      "Average loss at step  156000 :  5.46603936231\n",
      "Average loss at step  158000 :  5.58410956955\n",
      "Average loss at step  160000 :  5.46041093493\n",
      "Nearest to that: reinstall, whiteboards, pimped, imperative, popping, comparing, poof, assisted,\n",
      "Nearest to some: painters, snapshot, turrets, plungers, inconspicuous, many, shortage, deflating,\n",
      "Nearest to see: eat, visit, offbeat, neglect, giggled, 000, pedaled, crates,\n",
      "Nearest to the: silverstein, majestically, presenter, overtaking, turrets, apprentices, rotated, stan,\n",
      "Nearest to while: overtaken, hoodie, goddess, nos, lodging, trucked, bandana, neutralize,\n",
      "Nearest to water: announcement, ocean, firecrackers, emergencies, replicated, wrestlers, lawnmower, tactic,\n",
      "Nearest to everyone: everybody, principle, reminiscent, jewels, garcia, upward, trumpeters, healed,\n",
      "Nearest to talking: quenches, bewildered, trying, cheesesteak, collectibles, document, wes, confused,\n",
      "Nearest to food: shrubbery, wrestlers, edges, interrupted, conceded, passersby, arching, misfit,\n",
      "Nearest to had: have, fabricated, raceway, reznor, viper, medievel, simulated, icelandic,\n",
      "Nearest to went: go, drove, unimpressed, sit, tried, tho, swords, headed,\n",
      "Nearest to little: bit, dilapidation, beachgoers, geodesic, labelled, darkens, pillar, diverted,\n",
      "Nearest to into: through, toay, charms, implated, celsius, guides, prominant, constitution,\n",
      "Nearest to at: during, demands, persons, abyss, lance, la, pedraza, pooping,\n",
      "Nearest to as: gasping, viper, agh, infused, organizations, rockers, ramcharger, peeling,\n",
      "Nearest to then: hazel, turd, guinnes, inlcuding, whomever, uncool, prodigy, ny,\n",
      "Average loss at step  162000 :  5.74577032197\n",
      "Average loss at step  164000 :  5.61205546618\n",
      "Average loss at step  166000 :  5.54569759643\n",
      "Average loss at step  168000 :  5.4901870364\n",
      "Average loss at step  170000 :  5.52011023349\n",
      "Nearest to that: telus, aligning, shimmers, nose, pitchfork, spook, monochromatic, scholars,\n",
      "Nearest to some: yorkie, crowdsurfing, wakes, fisherman, fascinate, plungers, snowflakes, lounges,\n",
      "Nearest to see: find, read, keep, relationships, remember, watch, tell, impale,\n",
      "Nearest to the: inequality, our, fives, swapped, favourite, freehand, tastefully, financially,\n",
      "Nearest to while: after, neutralize, stretches, identify, peaking, shampoo, directed, peforming,\n",
      "Nearest to water: ocean, pier, announcement, crags, bot, boobs, city, esteemed,\n",
      "Nearest to everyone: jade, perfomed, finally, undrinkable, ahhed, snuffles, blueprints, dismayed,\n",
      "Nearest to talking: quenches, bewildered, walking, cheesesteak, wes, lite, disembodied, confused,\n",
      "Nearest to food: plains, goonies, jinkins, scrapper, bisque, buttons, exchanging, pastime,\n",
      "Nearest to had: have, tried, carnevil, approvingly, medievel, hound, strike, peta,\n",
      "Nearest to went: decided, go, headed, got, tried, tne, wanted, drop,\n",
      "Nearest to little: pillar, dilapidation, downside, cease, institute, cascades, flickered, fr,\n",
      "Nearest to into: through, roommates, towards, spoon, tye, lesson, overcast, reflective,\n",
      "Nearest to at: montage, thingy, whomever, joining, stool, outcast, reindeers, unmistakable,\n",
      "Nearest to as: calories, pans, sayuri, canal, unaccessible, nigh, inspected, curb,\n",
      "Nearest to then: scares, abbreviated, strictly, storybooks, buckets, whomever, pastures, workes,\n",
      "Average loss at step  172000 :  5.35737549055\n",
      "Average loss at step  174000 :  5.64940416563\n",
      "Average loss at step  176000 :  5.61099158657\n",
      "Average loss at step  178000 :  5.67940052938\n",
      "Average loss at step  180000 :  5.36686327815\n",
      "Nearest to that: when, whomever, adopting, sue, episode, aferward, requested, relays,\n",
      "Nearest to some: beuatiful, euros, stumbles, attacker, painters, plywood, superstitious, inequality,\n",
      "Nearest to see: meet, relationships, explore, flatnose, torah, defunct, stalk, watch,\n",
      "Nearest to the: opnion, crevice, braids, gregs, stormed, quieted, conservatively, paraded,\n",
      "Nearest to while: ami, slope, storyboard, libations, taga, directed, fumbles, necronomicon,\n",
      "Nearest to water: ocean, end, mountains, beach, city, pier, emergencies, monies,\n",
      "Nearest to everyone: braves, starbuck, permitted, foam, majors, upward, zinoa, disrupt,\n",
      "Nearest to talking: confused, quenches, climber, bewildered, happy, maddie, cheesesteak, talked,\n",
      "Nearest to food: plains, skyskrapers, spits, fuller, beaver, nearing, debark, dannys,\n",
      "Nearest to had: have, wanted, saw, took, earnest, challenging, bought, felix,\n",
      "Nearest to went: decided, drove, headed, go, chow, came, tried, suggested,\n",
      "Nearest to little: cascades, institute, downside, category, rubbish, small, quarantine, pillar,\n",
      "Nearest to into: through, wrecking, threating, in, rememberance, towards, renovations, vladimr,\n",
      "Nearest to at: morally, stool, birdie, vrooom, nightstand, choreographed, memoribilia, compact,\n",
      "Nearest to as: andersen, hotshot, repelled, damanged, increasing, guinnes, too, sue,\n",
      "Nearest to then: pinatas, misty, parasailing, ramcharger, pee, passersby, ambassador, scrumptious,\n",
      "Average loss at step  182000 :  5.6519890424\n",
      "Average loss at step  184000 :  5.56149253905\n",
      "Average loss at step  186000 :  5.3680253619\n",
      "Average loss at step  188000 :  5.28069229579\n",
      "Average loss at step  190000 :  5.55018835282\n",
      "Nearest to that: scholars, francisco, bearable, captivating, easel, douglass, safezones, hive,\n",
      "Nearest to some: euros, approval, offshore, buddha, blends, frighteningly, suave, gifted,\n",
      "Nearest to see: get, find, remember, tell, whistlers, angola, prove, make,\n",
      "Nearest to the: joseph, interlaces, leery, this, riddles, chateau, emotinal, dangle,\n",
      "Nearest to while: expansion, aka, peforming, cupful, thinly, gilt, butchered, assembles,\n",
      "Nearest to water: ocean, mountains, pier, city, beach, woods, hunk, lake,\n",
      "Nearest to everyone: braves, grasshoppers, he, everybody, tibetan, zinoa, ahhed, tying,\n",
      "Nearest to talking: talk, talked, walking, typing, relaxing, confused, rigors, shinensons,\n",
      "Nearest to food: grandeur, kindling, noticing, eggs, knott, spits, bu, belltower,\n",
      "Nearest to had: has, have, dislike, shopps, wanted, skateboarded, got, customize,\n",
      "Nearest to went: drove, walked, unimpressed, wass, sat, wanted, tried, biked,\n",
      "Nearest to little: bit, downside, asia, decent, rollercoasters, quarantine, peterson, whirl,\n",
      "Nearest to into: ray, dirtied, fullest, rememberance, threating, plummet, testicles, repaving,\n",
      "Nearest to at: sunbath, manned, resue, coax, jun, lu, falcon, 177,\n",
      "Nearest to as: because, sround, listeners, remembrances, lull, premium, vaccinated, jogger,\n",
      "Nearest to then: when, passersby, afterwards, bugged, gt, ambassador, shrines, troubleshooting,\n",
      "Average loss at step  192000 :  5.5091591121\n",
      "Average loss at step  194000 :  5.53657731962\n",
      "Average loss at step  196000 :  5.64727878141\n",
      "Average loss at step  198000 :  5.6128595413\n",
      "Average loss at step  200000 :  5.3512909106\n",
      "Nearest to that: prosperous, bedtime, wink, frown, cuz, raincoat, bearable, crushing,\n",
      "Nearest to some: deflating, jumpers, lots, oktober, carribean, fascinate, their, exclusive,\n",
      "Nearest to see: find, explore, leave, read, meet, tell, margarita, boomed,\n",
      "Nearest to the: malie, darts, abbeys, daffodils, superheroes, frescos, phillipo, pinecones,\n",
      "Nearest to while: jaw, shush, forthcoming, elect, butchered, unstoppable, galaxies, rigged,\n",
      "Nearest to water: lake, ocean, beach, kebabs, mountains, air, river, spaceman,\n",
      "Nearest to everyone: zinoa, mentioning, goofing, brulee, lightened, brrrr, pregnancy, dame,\n",
      "Nearest to talking: happy, breaking, confused, 35, climber, rigors, songwriters, emotinal,\n",
      "Nearest to food: grandeur, fish, spits, beauty, inequality, hipsterish, confessed, ethyl,\n",
      "Nearest to had: has, have, skateboarded, reznor, informal, needed, volume, secondly,\n",
      "Nearest to went: decided, go, amazes, sit, headed, franks, wanted, nectar,\n",
      "Nearest to little: young, emphasize, cute, wm, ralphy, violinist, greenhouse, toughing,\n",
      "Nearest to into: rememberance, outfield, ray, preschool, nom, cyclone, manuals, wang,\n",
      "Nearest to at: regarding, choke, agog, illuminated, sardo, delivery, longest, quieted,\n",
      "Nearest to as: soooooul, wording, vaccination, defending, andersen, frothy, highschool, smashed,\n",
      "Nearest to then: boogie, tt, mints, obey, starbursts, tick, murmurs, inclines,\n"
     ]
    }
   ],
   "source": [
    "#Begin training.\n",
    "num_steps = 200001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print(\"Initialized\")\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "    # We perform one update step by evaluating the optimizer op (including it\n",
    "    # in the list of returned values for session.run()\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print(\"Average loss at step \", step, \": \", average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "    # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = \"Nearest to %s:\" % valid_word\n",
    "                for k in xrange(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = \"%s %s,\" % (log_str, close_word)\n",
    "                print(log_str)\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Visualize the embeddings using PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "def plot_with_labels(low_dim_embs, labels, filename='pca.png'):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                     xy=(x, y),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "        plt.savefig(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##plot a random set of terms\n",
    "plot_only = 10000\n",
    "pca=PCA(n_components=2)\n",
    "x=pca.fit(final_embeddings[:plot_only, :])\n",
    "y=x.transform(final_embeddings[:plot_only, :])\n",
    "labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "plot_with_labels(y, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##save embeddings\n",
    "np.save(\"word_vector.npy\",final_embeddings)\n",
    "np.save(\"dictionary.npy\",dictionary)\n",
    "np.save(\"reverse_dictionary.npy\",reverse_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
